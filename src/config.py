    
from trlx.data.configs import (
    ModelConfig,
    OptimizerConfig,
    SchedulerConfig,
    TokenizerConfig,
    TrainConfig,
    TRLConfig,
)
from trlx.models.modeling_ppo import PPOConfig
from trlx.trainer.accelerate_sft_trainer import SFTConfig
import time

def default_t5_ppo_config(promptor, model_name_or_path="", learning_rate=0.):
    logging_dir=f'output/{time.time()}'
    config_dict = {
        "t5-large-ppo":TRLConfig(
            train=TrainConfig(
                tracker='tensorboard',
                logging_dir=logging_dir,
                seq_length=512,
                epochs=4318,
                total_steps=2**30,
                batch_size=24,
                eval_batch_size=64,
                checkpoint_interval=1280,
                eval_interval=32,
                pipeline="RecalledPipeline",
                eval_pipeline="SFTPipeline",
                trainer="AcceleratePPOTrainer",
                checkpoint_dir='./output',
                rollout_logging_dir='./output',
                skip_length=promptor.skip_length,
                pad_tokens=promptor.pad_tokens,
                sample_num=4,
            ),
            model=ModelConfig(
                model_path=model_name_or_path,
                model_arch_type="seq2seq",
                num_layers_unfrozen=4,
                delta_kwargs={"delta_type": "lora", "modified_modules": "all", "lora_r": 8, "lora_alpha": 16, "lora_dropout": 0.0},
            ),
            tokenizer=TokenizerConfig(
                tokenizer_path=model_name_or_path,
                truncation_side="right",
            ),
            optimizer=OptimizerConfig(
                name="adamw",
                kwargs={
                    "lr": learning_rate,
                    "betas": [0.9, 0.999],
                    "eps": 1.0e-8,
                    "weight_decay": 1.0e-6,
                },
            ),
            scheduler=SchedulerConfig(
                name="linear",
                # kwargs={
                #     "T_max": 1000,
                #     "eta_min": args.learning_rate/2,
                # },
            ),
            method=PPOConfig(
                name="PPOConfig",
                num_rollouts=24*32,
                chunk_size=12,
                ppo_epochs=1,
                init_kl_coef=0.05,
                target=6,
                horizon=10000,
                gamma=0.99,
                lam=0.95,
                cliprange=0.2,
                cliprange_value=0.2,
                vf_coef=1.0,
                scale_reward=None,
                ref_mean=None,
                ref_std=None,
                cliprange_reward=10,
                gen_kwargs={
                    "min_length": 4,
                    "do_sample": False,
                    "max_new_tokens": promptor.mean_qry_len+16,
                    "num_return_sequences": 1, 
                },
                gen_experience_kwargs={
                    "min_length": 4,
                    "max_new_tokens": promptor.mean_qry_len+16,
                    "do_sample": False,
                    # "temperature": 0.7,
                    # "top_k": 5,
                    # "top_p": 0.95,
                },
            ),
        )
        , "xxl-4ppo":TRLConfig(
            train=TrainConfig(
                tracker='tensorboard',
                logging_dir=logging_dir,
                seq_length=200,
                epochs=4318,
                total_steps=2**30,
                batch_size=16,
                eval_batch_size=128,
                checkpoint_interval=1280,
                eval_interval=256,
                pipeline="RecalledPipeline",
                eval_pipeline="SFTPipeline",
                trainer="AcceleratePPOTrainer",
                checkpoint_dir='./output',
                rollout_logging_dir='./output',
                skip_length=promptor.skip_length,
                pad_tokens=promptor.pad_tokens,
                sample_num=4,
            ),
            model=ModelConfig(
                model_path=model_name_or_path,
                model_arch_type="seq2seq",
                num_layers_unfrozen=2,
            ),
            tokenizer=TokenizerConfig(
                tokenizer_path=model_name_or_path,
                truncation_side="right",
            ),
            optimizer=OptimizerConfig(
                name="adamw",
                kwargs={
                    "lr": learning_rate,
                    "betas": [0.9, 0.999],
                    "eps": 1.0e-8,
                    "weight_decay": 1.0e-6,
                },
            ),
            scheduler=SchedulerConfig(
                name="linear",
                # kwargs={
                #     "T_max": 1000,
                #     "eta_min": args.learning_rate/2,
                # },
            ),
            method=PPOConfig(
                name="PPOConfig",
                num_rollouts=16*32,
                chunk_size=4,
                ppo_epochs=1,
                init_kl_coef=0.05,
                target=6,
                horizon=10000,
                gamma=0.99,
                lam=0.95,
                cliprange=0.2,
                cliprange_value=0.2,
                vf_coef=1.0,
                scale_reward=None,
                ref_mean=None,
                ref_std=None,
                cliprange_reward=10,
                gen_kwargs={
                    "min_length": 4,
                    "max_new_tokens": promptor.mean_qry_len+16,
                    "num_return_sequences": 1, 
                },
                gen_experience_kwargs={
                    "min_length": 4,
                    "max_new_tokens": promptor.mean_qry_len+16,
                    "do_sample": False,
                    # "temperature": 0.7,
                    # "top_k": 5,
                    # "top_p": 0.95,
                },
            ),
        ), "xl":TRLConfig(
            train=TrainConfig(
                tracker='tensorboard',
                logging_dir=logging_dir,
                seq_length=200,
                epochs=4318,
                total_steps=2**30,
                batch_size=64,
                eval_batch_size=512,
                checkpoint_interval=1280,
                eval_interval=256,
                pipeline="RecalledPipeline",
                eval_pipeline="SFTPipeline",
                trainer="AcceleratePPOTrainer",
                checkpoint_dir='./output',
                rollout_logging_dir='./output',
                skip_length=promptor.skip_length,
                pad_tokens=promptor.pad_tokens,
                sample_num=4,
            ),
            model=ModelConfig(
                model_path=model_name_or_path,
                model_arch_type="seq2seq",
                num_layers_unfrozen=2,
            ),
            tokenizer=TokenizerConfig(
                tokenizer_path=model_name_or_path,
                truncation_side="right",
            ),
            optimizer=OptimizerConfig(
                name="adamw",
                kwargs={
                    "lr": learning_rate,
                    "betas": [0.9, 0.999],
                    "eps": 1.0e-8,
                    "weight_decay": 1.0e-6,
                },
            ),
            scheduler=SchedulerConfig(
                name="linear",
                # kwargs={
                #     "T_max": 1000,
                #     "eta_min": args.learning_rate/2,
                # },
            ),
            method=PPOConfig(
                name="PPOConfig",
                num_rollouts=64*32,
                chunk_size=32,
                ppo_epochs=1,
                init_kl_coef=0.05,
                target=6,
                horizon=10000,
                gamma=0.99,
                lam=0.95,
                cliprange=0.2,
                cliprange_value=0.2,
                vf_coef=1.0,
                scale_reward=None,
                ref_mean=None,
                ref_std=None,
                cliprange_reward=10,
                gen_kwargs={
                    "min_length": 4,
                    "max_new_tokens": promptor.mean_qry_len+16,
                    "num_return_sequences": 1, 
                },
                gen_experience_kwargs={
                    "min_length": 4,
                    "max_new_tokens": promptor.mean_qry_len+16,
                    "do_sample": False,
                    # "temperature": 0.7,
                    # "top_k": 5,
                    # "top_p": 0.95,
                },
            ),
        ), "ul2":TRLConfig(
            train=TrainConfig(
                tracker='tensorboard',
                logging_dir=logging_dir,
                seq_length=200,
                epochs=4318,
                total_steps=2**30,
                batch_size=32,
                eval_batch_size=256,
                checkpoint_interval=1280,
                eval_interval=256,
                pipeline="RecalledPipeline",
                eval_pipeline="SFTPipeline",
                trainer="AcceleratePPOTrainer",
                checkpoint_dir='./output',
                rollout_logging_dir='./output',
                skip_length=promptor.skip_length,
                pad_tokens=promptor.pad_tokens,
                sample_num=4,
            ),
            model=ModelConfig(
                model_path=model_name_or_path,
                model_arch_type="seq2seq",
                num_layers_unfrozen=0,
            ),
            tokenizer=TokenizerConfig(
                tokenizer_path=model_name_or_path,
                truncation_side="right",
            ),
            optimizer=OptimizerConfig(
                name="adamw",
                kwargs={
                    "lr": learning_rate,
                    "betas": [0.9, 0.999],
                    "eps": 1.0e-8,
                    "weight_decay": 1.0e-6,
                },
            ),
            scheduler=SchedulerConfig(
                name="linear",
                # kwargs={
                #     "T_max": 1000,
                #     "eta_min": args.learning_rate/2,
                # },
            ),
            method=PPOConfig(
                name="PPOConfig",
                num_rollouts=32*64,
                chunk_size=32,
                ppo_epochs=1,
                init_kl_coef=0.05,
                target=6,
                horizon=10000,
                gamma=0.99,
                lam=0.95,
                cliprange=0.2,
                cliprange_value=0.2,
                vf_coef=1.0,
                scale_reward=None,
                ref_mean=None,
                ref_std=None,
                cliprange_reward=10,
                gen_kwargs={
                    "min_length": 4,
                    "max_new_tokens": promptor.mean_qry_len+16,
                    "num_return_sequences": 1, 
                },
                gen_experience_kwargs={
                    "min_length": 4,
                    "max_new_tokens": promptor.mean_qry_len+16,
                    "do_sample": False,
                    # "temperature": 0.7,
                    # "top_k": 5,
                    # "top_p": 0.95,
                },
            ),
        ), 't5-xxl-inference':TRLConfig(
        train=TrainConfig(
            tracker='tensorboard',
            logging_dir='output',
            seq_length=200,
            epochs=100,
            total_steps=2**30,
            batch_size=32,
            eval_batch_size=32,
            checkpoint_interval=1280,
            eval_interval=128,
            pipeline="SFTPipeline",
            eval_pipeline="SFTPipeline",
            trainer="AccelerateSFTTrainer",
            checkpoint_dir='./output',
            rollout_logging_dir='./output',
            skip_length=promptor.skip_length,
            pad_tokens=promptor.pad_tokens,
        ),
        model=ModelConfig(
            model_path=model_name_or_path,
            model_arch_type="seq2seq",
            num_layers_unfrozen=1,
        ),
        tokenizer=TokenizerConfig(
            tokenizer_path=model_name_or_path,
            truncation_side="right",
        ),
        optimizer=OptimizerConfig(
            name="adamw",
            kwargs={
                "lr": learning_rate,
                "betas": [0.9, 0.999],
                "eps": 1.0e-8,
                "weight_decay": 1.0e-6,
            },
        ),
        scheduler=SchedulerConfig(
            name="cosine_annealing",
            kwargs={
                "T_max": 1000,
                "eta_min": learning_rate/2,
            },
        ),
        method=SFTConfig(
            name="SFTConfig",
            gen_kwargs={
                "min_length": 4,
                "max_new_tokens": promptor.mean_qry_len+16,
                "do_sample": False,
                "temperature": 1.0,
                "top_k": 1,
                "suppress_tokens":[[564, 800, 1053, 27569, 11860, 166, 2233, 8986, 1108, 1448, 2859, 5454, 1708, 1730, 543, 711]],
                # "no_repeat_ngram_size": 1,
                "num_return_sequences": 1, 
                # "encoder_no_repeat_ngram_size": 2,
            },
        ),
    ), 't5-xl-inference':TRLConfig(
        train=TrainConfig(
            tracker='tensorboard',
            logging_dir='output',
            seq_length=200,
            epochs=100,
            total_steps=2**30,
            batch_size=32,
            eval_batch_size=128,
            checkpoint_interval=1280,
            eval_interval=128,
            pipeline="SFTPipeline",
            eval_pipeline="SFTPipeline",
            trainer="AccelerateSFTTrainer",
            checkpoint_dir='./output',
            rollout_logging_dir='./output',
            skip_length=promptor.skip_length,
            pad_tokens=promptor.pad_tokens,
        ),
        model=ModelConfig(
                model_path=model_name_or_path,
                checkpoint_dir='',
                model_arch_type="seq2seq",
                num_layers_unfrozen=23,
                delta_kwargs={"delta_type": "lora", "modified_modules": "all", "lora_r": 8, "lora_alpha": 16, "lora_dropout": 0.0},
            ),
        tokenizer=TokenizerConfig(
            tokenizer_path=model_name_or_path,
            truncation_side="right",
        ),
        optimizer=OptimizerConfig(
            name="adamw",
            kwargs={
                "lr": learning_rate,
                "betas": [0.9, 0.999],
                "eps": 1.0e-8,
                "weight_decay": 1.0e-6,
            },
        ),
        scheduler=SchedulerConfig(
            name="cosine_annealing",
            kwargs={
                "T_max": 1000,
                "eta_min": learning_rate/2,
            },
        ),
        method=SFTConfig(
            name="SFTConfig",
            gen_kwargs={
                "min_length": 4,
                "max_new_tokens": promptor.mean_qry_len+8,
                "do_sample": False,
                "temperature": 1.0,
                "top_k": 1,
                "suppress_tokens":[[564, 800, 1053, 27569, 11860, 166, 2233, 8986, 1108, 1448, 2859, 5454, 1708, 1730, 543, 711]],
                # "no_repeat_ngram_size": 1,
                "num_return_sequences": 1, 
                # "encoder_no_repeat_ngram_size": 2,
            },
        ),
    ), 'llama-7b-ppo':TRLConfig(
        train=TrainConfig(
            tracker='tensorboard',
            logging_dir=logging_dir,
            seq_length=200,
            epochs=17269,
            total_steps=2**30,
            batch_size=4,
            eval_batch_size=12,
            checkpoint_interval=1280,
            eval_interval=64,
            pipeline="RecalledPipeline",
            eval_pipeline="SFTPipeline",
            trainer="AcceleratePPOTrainer",
            checkpoint_dir='./output',
            rollout_logging_dir='./output',
            skip_length=promptor.skip_length,
            pad_tokens=promptor.pad_tokens,
            sample_num=4,
        ),
        model=ModelConfig(
            model_path=model_name_or_path,
            model_arch_type="causal",
            num_layers_unfrozen=1,
            delta_kwargs={"delta_type": "lora", "modified_modules": "all", "lora_r": 8, "lora_alpha": 16, "lora_dropout": 0.0},
        ),
        tokenizer=TokenizerConfig(
            tokenizer_path=model_name_or_path,
            truncation_side="right",
        ),
        optimizer=OptimizerConfig(
            name="adamw",
            kwargs={
                "lr": learning_rate,
                "betas": [0.9, 0.999],
                "eps": 1.0e-8,
                "weight_decay": 1.0e-6,
            },
        ),
        scheduler=SchedulerConfig(
                name="linear",
                # kwargs={
                #     "T_max": 1000,
                #     "eta_min": args.learning_rate/2,
                # },
        ),
        method=PPOConfig(
                name="PPOConfig",
                num_rollouts=4*32,
                chunk_size=1,
                ppo_epochs=1,
                init_kl_coef=0.05,
                target=6,
                horizon=10000,
                gamma=0.99,
                lam=0.95,
                cliprange=0.2,
                cliprange_value=0.2,
                vf_coef=1.0,
                scale_reward=None,
                ref_mean=None,
                ref_std=None,
                cliprange_reward=10,
                gen_kwargs={
                    "min_length": 4,
                    "max_new_tokens": 128,
                    "num_return_sequences": 1, 
                },
                gen_experience_kwargs={
                    "min_length": 4,
                    "max_new_tokens": 128,
                    "do_sample": False,
                    # "temperature": 0.7,
                    # "top_k": 5,
                    # "top_p": 0.95,
                },
            ),
    ),'t5-large-inference':TRLConfig(
        train=TrainConfig(
            tracker='tensorboard',
            logging_dir='output',
            seq_length=200,
            epochs=100,
            total_steps=2**30,
            batch_size=32,
            eval_batch_size=700,
            checkpoint_interval=1280,
            eval_interval=128,
            pipeline="SFTPipeline",
            eval_pipeline="SFTPipeline",
            trainer="AccelerateSFTTrainer",
            checkpoint_dir='./output',
            rollout_logging_dir='./output',
            skip_length=promptor.skip_length,
            pad_tokens=promptor.pad_tokens,
        ),
        model=ModelConfig(
            model_path=model_name_or_path,
            model_arch_type="seq2seq",
            num_layers_unfrozen=1,
        ),
        tokenizer=TokenizerConfig(
            tokenizer_path=model_name_or_path,
            truncation_side="right",
        ),
        optimizer=OptimizerConfig(
            name="adamw",
            kwargs={
                "lr": learning_rate,
                "betas": [0.9, 0.999],
                "eps": 1.0e-8,
                "weight_decay": 1.0e-6,
            },
        ),
        scheduler=SchedulerConfig(
            name="cosine_annealing",
            kwargs={
                "T_max": 1000,
                "eta_min": learning_rate/2,
            },
        ),
        method=SFTConfig(
            name="SFTConfig",
            gen_kwargs={
                "min_length": 4,
                "max_new_tokens": promptor.mean_qry_len+16,
                "do_sample": False,
                "temperature": 1.0,
                "top_k": 1,
                # "no_repeat_ngram_size": 1,
                "num_return_sequences": 1, 
                # "encoder_no_repeat_ngram_size": 2,
            },
        ),
    ),
    "t5-xl-ppo":TRLConfig(
            train=TrainConfig(
                tracker='tensorboard',
                logging_dir=logging_dir,
                seq_length=200,
                epochs=4318,
                total_steps=40000,
                batch_size=32,
                eval_batch_size=64,
                checkpoint_interval=1280,
                eval_interval=32,
                pipeline="RecalledPipeline",
                eval_pipeline="SFTPipeline",
                trainer="AcceleratePPOTrainer",
                checkpoint_dir='./output',
                rollout_logging_dir='./output',
                sample_num=4,
                skip_length=promptor.skip_length,
                pad_tokens=promptor.pad_tokens,
            ),
            model=ModelConfig(
                model_path=model_name_or_path,
                checkpoint_dir='',
                model_arch_type="seq2seq",
                num_layers_unfrozen=23,
                delta_kwargs={"delta_type": "lora", "modified_modules": "all", "lora_r": 8, "lora_alpha": 16, "lora_dropout": 0.0},
            ),
            tokenizer=TokenizerConfig(
                tokenizer_path=model_name_or_path,
                truncation_side="right",
            ),
            optimizer=OptimizerConfig(
                name="adamw",
                kwargs={
                    "lr": learning_rate,
                    "betas": [0.9, 0.999],
                    "eps": 1.0e-8,
                    "weight_decay": 1.0e-6,
                },
            ),
            scheduler=SchedulerConfig(
                name="linear",
                # kwargs={
                #     "T_max": 1000,
                #     "eta_min": args.learning_rate/2,
                # },
            ),
            method=PPOConfig(
                name="PPOConfig",
                num_rollouts=32*32,
                chunk_size=16,
                ppo_epochs=1,
                init_kl_coef=0.05,
                target=6,
                horizon=10000,
                gamma=0.99,
                lam=0.95,
                cliprange=0.2,
                cliprange_value=0.2,
                vf_coef=1.0,
                scale_reward=None,
                ref_mean=None,
                ref_std=None,
                cliprange_reward=10,
                gen_kwargs={
                    "min_length": 4,
                    "max_new_tokens": promptor.mean_qry_len+8,
                    "do_sample": False,
                    "num_return_sequences": 1, 
                    "suppress_tokens":[[564, 800, 1053, 27569, 11860, 166, 2233, 8986, 1108, 1448, 2859, 5454, 1708, 1730, 543, 711]],
                },
                gen_experience_kwargs={
                    "min_length": 4,
                    "max_new_tokens": promptor.mean_qry_len+8,
                    "do_sample": False,
                    "suppress_tokens":[[564, 800, 1053, 27569, 11860, 166, 2233, 8986, 1108, 1448, 2859, 5454, 1708, 1730, 543, 711]],
                    "temperature": 0.7,
                    "top_k": 5,
                    "top_p": 0.95,
                },
            ),
        ), 
    'chatglm-inference':TRLConfig(
        train=TrainConfig(
            tracker='tensorboard',
            logging_dir='output',
            seq_length=1024,
            epochs=100,
            total_steps=2**30,
            batch_size=32,
            eval_batch_size=16,
            checkpoint_interval=1280,
            eval_interval=128,
            pipeline="SFTPipeline",
            eval_pipeline="SFTPipeline",
            trainer="AccelerateSFTTrainer",
            checkpoint_dir='./output',
            rollout_logging_dir='./output',
            skip_length=promptor.skip_length,
            pad_tokens=promptor.pad_tokens,
            sample_num=4,
        ),
        model=ModelConfig(
            model_path=model_name_or_path,
            model_arch_type="causal",
            num_layers_unfrozen=1,
        ),
        tokenizer=TokenizerConfig(
            tokenizer_path=model_name_or_path,
            truncation_side="left",
            padding_side="left",
        ),
        optimizer=OptimizerConfig(
            name="adamw",
            kwargs={
                "lr": learning_rate,
                "betas": [0.9, 0.999],
                "eps": 1.0e-8,
                "weight_decay": 1.0e-6,
            },
        ),
        scheduler=SchedulerConfig(
            name="cosine_annealing",
            kwargs={
                "T_max": 1000,
                "eta_min": learning_rate/2,
            },
        ),
        method=SFTConfig(
            name="SFTConfig",
            gen_kwargs={
                "min_length": 4,
                "max_new_tokens": 512,
                "do_sample": False,
                "temperature": 1.0,
                "top_k": 1,
                "no_repeat_ngram_size": 6,
                "num_return_sequences": 1, 
                # "encoder_no_repeat_ngram_size": 2,
            },
        ),

    ),
    "chatglm-ppo":TRLConfig(
            train=TrainConfig(
                tracker='tensorboard',
                logging_dir=logging_dir,
                seq_length=200,
                epochs=4318,
                total_steps=4000,
                batch_size=4,
                eval_batch_size=12,
                checkpoint_interval=1280,
                eval_interval=32,
                pipeline="RecalledPipeline",
                eval_pipeline="SFTPipeline",
                trainer="AcceleratePPOTrainer",
                checkpoint_dir='./output',
                rollout_logging_dir='./output',
                sample_num=4,
                skip_length=promptor.skip_length,
                pad_tokens=promptor.pad_tokens,
            ),
            model=ModelConfig(
                model_path=model_name_or_path,
                checkpoint_dir='',
                model_arch_type="causal",
                num_layers_unfrozen=1,
                # delta_kwargs={"delta_type": "lora", "modified_modules": "mlp", "lora_r": 8, "lora_alpha": 16, "lora_dropout": 0.0},
            ),
            tokenizer=TokenizerConfig(
                tokenizer_path=model_name_or_path,
                truncation_side="right",
            ),
            optimizer=OptimizerConfig(
                name="adamw",
                kwargs={
                    "lr": learning_rate,
                    "betas": [0.9, 0.999],
                    "eps": 1.0e-8,
                    "weight_decay": 1.0e-6,
                },
            ),
            scheduler=SchedulerConfig(
                name="linear",
                # kwargs={
                #     "T_max": 1000,
                #     "eta_min": args.learning_rate/2,
                # },
            ),
            method=PPOConfig(
                name="PPOConfig",
                num_rollouts=4*32,
                chunk_size=1,
                ppo_epochs=1,
                init_kl_coef=0.05,
                target=6,
                horizon=10000,
                gamma=0.99,
                lam=0.95,
                cliprange=0.2,
                cliprange_value=0.2,
                vf_coef=1.0,
                scale_reward=None,
                ref_mean=None,
                ref_std=None,
                cliprange_reward=10,
                gen_kwargs={
                    "min_length": 4,
                    "max_new_tokens": promptor.mean_qry_len+8,
                    "do_sample": False,
                    "num_return_sequences": 1, 
                },
                gen_experience_kwargs={
                    "min_length": 4,
                    "max_new_tokens": promptor.mean_qry_len+8,
                    "do_sample": False,
                    "temperature": 0.7,
                    "top_k": 5,
                    "top_p": 0.95,
                },
            ),
        ), 
        'BELLE-7B-2M-ppo':TRLConfig(
        train=TrainConfig(
            tracker='tensorboard',
            logging_dir=logging_dir,
            seq_length=200,
            epochs=17269,
            total_steps=2**30,
            batch_size=8,
            eval_batch_size=16,
            checkpoint_interval=1280,
            eval_interval=32,
            pipeline="RecalledPipeline",
            eval_pipeline="SFTPipeline",
            trainer="AcceleratePPOTrainer",
            checkpoint_dir='./output',
            rollout_logging_dir='./output',
            skip_length=promptor.skip_length,
            pad_tokens=promptor.pad_tokens,
            sample_num=4,
        ),
        model=ModelConfig(
            model_path=model_name_or_path,
            model_arch_type="causal",
            num_layers_unfrozen=12,
            delta_kwargs={"delta_type": "lora", "modified_modules": "all", "lora_r": 8, "lora_alpha": 16, "lora_dropout": 0.0},
        ),
        tokenizer=TokenizerConfig(
            tokenizer_path=model_name_or_path,
            truncation_side="right",
        ),
        optimizer=OptimizerConfig(
            name="adamw",
            kwargs={
                "lr": learning_rate,
                "betas": [0.9, 0.999],
                "eps": 1.0e-8,
                "weight_decay": 1.0e-6,
            },
        ),
        scheduler=SchedulerConfig(
                name="linear",
                # kwargs={
                #     "T_max": 1000,
                #     "eta_min": args.learning_rate/2,
                # },
        ),
        method=PPOConfig(
                name="PPOConfig",
                num_rollouts=8*32,
                chunk_size=2,
                ppo_epochs=1,
                init_kl_coef=0.05,
                target=6,
                horizon=10000,
                gamma=0.99,
                lam=0.95,
                cliprange=0.2,
                cliprange_value=0.2,
                vf_coef=1.0,
                scale_reward=None,
                ref_mean=None,
                ref_std=None,
                cliprange_reward=10,
                gen_kwargs={
                    "min_length": 4,
                    "max_new_tokens": 32,
                    "num_return_sequences": 1, 
                },
                gen_experience_kwargs={
                    "min_length": 4,
                    "max_new_tokens": 32,
                    "do_sample": False,
                    # "temperature": 0.7,
                    # "top_k": 5,
                    # "top_p": 0.95,
                },
            ),
    ),
    'llama-13B-ppo':TRLConfig(
        train=TrainConfig(
            tracker='tensorboard',
            logging_dir=logging_dir,
            seq_length=200,
            epochs=17269,
            total_steps=2**30,
            batch_size=4,
            eval_batch_size=8,
            checkpoint_interval=1280,
            eval_interval=32,
            pipeline="RecalledPipeline",
            eval_pipeline="SFTPipeline",
            trainer="AcceleratePPOTrainer",
            checkpoint_dir='./output',
            rollout_logging_dir='./output',
            skip_length=promptor.skip_length,
            pad_tokens=promptor.pad_tokens,
            sample_num=4,
        ),
        model=ModelConfig(
            model_path=model_name_or_path,
            model_arch_type="causal",
            num_layers_unfrozen=4,
            delta_kwargs={"delta_type": "lora", "modified_modules": "all", "lora_r": 8, "lora_alpha": 16, "lora_dropout": 0.0},
        ),
        tokenizer=TokenizerConfig(
            tokenizer_path=model_name_or_path,
            truncation_side="right",
        ),
        optimizer=OptimizerConfig(
            name="adamw",
            kwargs={
                "lr": learning_rate,
                "betas": [0.9, 0.999],
                "eps": 1.0e-8,
                "weight_decay": 1.0e-6,
            },
        ),
        scheduler=SchedulerConfig(
                name="linear",
                # kwargs={
                #     "T_max": 1000,
                #     "eta_min": args.learning_rate/2,
                # },
        ),
        method=PPOConfig(
                name="PPOConfig",
                num_rollouts=4*32,
                chunk_size=1,
                ppo_epochs=1,
                init_kl_coef=0.05,
                target=6,
                horizon=10000,
                gamma=0.99,
                lam=0.95,
                cliprange=0.2,
                cliprange_value=0.2,
                vf_coef=1.0,
                scale_reward=None,
                ref_mean=None,
                ref_std=None,
                cliprange_reward=10,
                gen_kwargs={
                    "min_length": 4,
                    "max_new_tokens": 32,
                    "num_return_sequences": 1, 
                },
                gen_experience_kwargs={
                    "min_length": 4,
                    "max_new_tokens": 32,
                    "do_sample": False,
                    # "temperature": 0.7,
                    # "top_k": 5,
                    # "top_p": 0.95,
                },
            ),
    ),
    "t5-xxl-ppo":TRLConfig(
            train=TrainConfig(
                tracker='tensorboard',
                logging_dir=logging_dir,
                seq_length=200,
                epochs=4318,
                total_steps=40000,
                batch_size=8,
                eval_batch_size=24,
                checkpoint_interval=1280,
                eval_interval=32,
                pipeline="RecalledPipeline",
                eval_pipeline="SFTPipeline",
                trainer="AcceleratePPOTrainer",
                checkpoint_dir='./output',
                rollout_logging_dir='./output',
                sample_num=4,
                skip_length=promptor.skip_length,
                pad_tokens=promptor.pad_tokens,
            ),
            model=ModelConfig(
                model_path=model_name_or_path,
                checkpoint_dir='',
                model_arch_type="seq2seq",
                num_layers_unfrozen=4,
                delta_kwargs={"delta_type": "lora", "modified_modules": "all", "lora_r": 8, "lora_alpha": 16, "lora_dropout": 0.0},
            ),
            tokenizer=TokenizerConfig(
                tokenizer_path=model_name_or_path,
                truncation_side="right",
            ),
            optimizer=OptimizerConfig(
                name="adamw",
                kwargs={
                    "lr": learning_rate,
                    "betas": [0.9, 0.999],
                    "eps": 1.0e-8,
                    "weight_decay": 1.0e-6,
                },
            ),
            scheduler=SchedulerConfig(
                name="linear",
                # kwargs={
                #     "T_max": 1000,
                #     "eta_min": args.learning_rate/2,
                # },
            ),
            method=PPOConfig(
                name="PPOConfig",
                num_rollouts=8*32,
                chunk_size=2,
                ppo_epochs=1,
                init_kl_coef=0.05,
                target=6,
                horizon=10000,
                gamma=0.99,
                lam=0.95,
                cliprange=0.2,
                cliprange_value=0.2,
                vf_coef=1.0,
                scale_reward=None,
                ref_mean=None,
                ref_std=None,
                cliprange_reward=10,
                gen_kwargs={
                    "min_length": 4,
                    "max_new_tokens": promptor.mean_qry_len+8,
                    "do_sample": False,
                    "num_return_sequences": 1, 
                    "suppress_tokens":[[564, 800, 1053, 27569, 11860, 166, 2233, 8986, 1108, 1448, 2859, 5454, 1708, 1730, 543, 711]],
                },
                gen_experience_kwargs={
                    "min_length": 4,
                    "max_new_tokens": promptor.mean_qry_len+8,
                    "do_sample": False,
                    "suppress_tokens":[[564, 800, 1053, 27569, 11860, 166, 2233, 8986, 1108, 1448, 2859, 5454, 1708, 1730, 543, 711]],
                    "temperature": 0.7,
                    "top_k": 5,
                    "top_p": 0.95,
                },
            ),
        ), 
    }
    return config_dict['t5-xl-inference']

